# ğŸ® Connectâ€‘4 AI 

A **fullâ€‘stack Connectâ€‘4 game** featuring **multiple AI difficulty levels**, including a **Deep Reinforcement Learning (DQN) agent trained to challenge and beat Minimax**.

Built with:
- âš¡ **FastAPI** backend
- ğŸ§  **PyTorch (DQN)** for Hard AI
- â™Ÿ **Minimax Algorithm** for Medium AI
- ğŸ¨ **HTML, CSS, JavaScript** frontend

---

## ğŸš€ Live Features

### ğŸ¯ Game Modes
| Difficulty | AI Type | Description |
|---------|--------|-------------|
| Easy | Random AI | Plays random valid moves |
| Medium | Minimax | Strategic searchâ€‘based AI |
| Hard | DQN (Neural Network) | Trained using Reinforcement Learning |

### ğŸ§  Hard AI Highlights
- Deep Qâ€‘Network (DQN)
- Experience Replay
- Target Network
- Epsilonâ€‘Greedy Exploration
- Trained over **100,000+ episodes**

---

## ğŸ—‚ Project Structure

```
root/
â”‚
â”œâ”€â”€ main.py                  # FastAPI server
â”œâ”€â”€ requirements.txt         # Dependencies
â”œâ”€â”€ dqn_model.pt             # Trained DQN weights
â”‚
â”œâ”€â”€ ai/
â”‚   â”œâ”€â”€ dqn_model.py         # Neural network architecture
â”‚   â”œâ”€â”€ hard_ai.py           # DQN inference logic
â”‚   â”œâ”€â”€ minimax_ai.py        # Medium difficulty AI
â”‚   â”œâ”€â”€ random_ai.py         # Easy difficulty AI
â”‚   â”œâ”€â”€ replay_buffer.py     # Experience replay
â”‚   â””â”€â”€ train_dqn.py         # Training script
â”‚
â”œâ”€â”€ game/
â”‚   â”œâ”€â”€ board.py             # Board representation & moves
â”‚   â”œâ”€â”€ rules.py             # Win & draw logic
â”‚   â””â”€â”€ utils.py             # Valid move helpers
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html           # UI
â”‚   â”œâ”€â”€ style.css            # Styling
â”‚   â””â”€â”€ script.js            # Frontend logic
```

---

## ğŸ§ª How the Hard AI Works (DQN)

1. Board is flattened into a **42â€‘dim state vector**
2. Neural network outputs **Qâ€‘values for 7 columns**
3. Invalid moves are masked
4. Highest Qâ€‘value valid move is chosen

### Training Strategy
- AI plays against random / curriculum opponents
- Rewards:
  - `+1` â†’ Win
  - `-1` â†’ Loss
  - `+0.2` â†’ Draw
  - `-0.01` â†’ Every move (to encourage faster wins)

---

## ğŸ–¥ Running Locally

### 1ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Start the server

```bash
uvicorn main:app --reload
```

### 3ï¸âƒ£ Open browser

```
http://127.0.0.1:8000
```

---

## ğŸ§  Training the DQN (Optional)

```bash
python ai/train_dqn.py
```

You can safely stop training anytime â€” the model is saved incrementally.

---

## ğŸ“¦ Model Details

- Framework: **PyTorch**
- File size: ~**436 KB**
- Architecture: Fullyâ€‘connected DQN

---

## ğŸ† Why This Project is Special

âœ… Combines **classic algorithms + deep learning**

âœ… Productionâ€‘ready backend & frontend

âœ… Deployable on cloud platforms

âœ… Educational + competitive gameplay

---

## ğŸ”® Future Improvements

- Curriculum learning (Minimax opponent during training)
- Selfâ€‘play DQN vs DQN
- AlphaZeroâ€‘style MCTS
- Mobile UI
- Multiplayer mode

---

## ğŸ‘¤ Author

Built with â¤ï¸ by **Itami**

If you like this project, â­ it on GitHub!

